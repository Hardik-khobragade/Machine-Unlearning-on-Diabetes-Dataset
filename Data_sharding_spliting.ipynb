{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f44098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5d208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as 'diabetes_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "# Add target column\n",
    "df['target'] = diabetes.target\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('diabetes_with_users_reordered.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved as 'diabetes_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f49a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: DATA PREPROCESSOR\n",
    "# =======================================================\n",
    "def _detect_user_id_column(dataframe: pd.DataFrame) -> str:\n",
    "    \"\"\"Helper function to find a user ID column.\"\"\"\n",
    "    possible_names = ['user_id', 'userid', 'user', 'uid', 'id']\n",
    "    lower_cols = {col.lower(): col for col in dataframe.columns}\n",
    "    for name in possible_names:\n",
    "        if name in lower_cols:\n",
    "            return lower_cols[name]\n",
    "    # Raise error only if no column is found, to be caught below\n",
    "    raise ValueError(\"No user identifier column found.\")\n",
    "\n",
    "def prepare_dataset(filepath):\n",
    "    #Loads, cleans, and prepares the dataset.\n",
    "    print(\" Running data preprocessing...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(\" Dataset loaded successfully\")\n",
    "\n",
    "    if df.isnull().values.any():\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "    else:\n",
    "        print(\"No missing values found\")\n",
    "\n",
    "    non_numeric_cols = df.select_dtypes(exclude=['number']).columns\n",
    "    if len(non_numeric_cols) > 0:\n",
    "        encoder = LabelEncoder()\n",
    "        for col in non_numeric_cols:\n",
    "            df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "\n",
    "    # Try to find a user_id column. If not found, create one.\n",
    "    try:\n",
    "        user_id_col = _detect_user_id_column(df)\n",
    "        print(f\" Existing user ID column ('{user_id_col}') found\")\n",
    "    except ValueError:\n",
    "        print(\" No user ID column found. Creating a new 'user_id' column\")\n",
    "        df[\"user_id\"] = np.arange(1, len(df) + 1)\n",
    "\n",
    "    df[\"index\"] = df.index\n",
    "    print(\"  - Preprocessing complete\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6148798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: SISA LOGIC\n",
    "# =======================================================\n",
    "def create_splits_and_mapping(dataframe, num_shards, splits_per_shard):\n",
    "    \"\"\"Creates shards, splits, and a DETAILED mapping of users to their data locations.\"\"\"\n",
    "    print(\"\\n Running SISA splitting and mapping\")\n",
    "    user_id_col = _detect_user_id_column(dataframe)\n",
    "    shuffled_df = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    shards = np.array_split(shuffled_df, num_shards)\n",
    "\n",
    "    all_splits = []\n",
    "    user_mapping = {}\n",
    "\n",
    "    for shard_idx, shard_df in enumerate(shards):\n",
    "        if shard_df.empty: continue\n",
    "        splits = np.array_split(shard_df, splits_per_shard)\n",
    "\n",
    "        for split_idx, split_df in enumerate(splits):\n",
    "            if split_df.empty: continue\n",
    "            all_splits.append(split_df)\n",
    "\n",
    "            for _, row in split_df.iterrows():\n",
    "                user_id = int(row[user_id_col])\n",
    "                original_index = int(row['index'])\n",
    "\n",
    "                if user_id not in user_mapping:\n",
    "                    user_mapping[user_id] = {\n",
    "                        'original_rows': [],\n",
    "                        'locations': {}\n",
    "                    }\n",
    "\n",
    "                user_mapping[user_id]['original_rows'].append(original_index)\n",
    "\n",
    "                location_key = (shard_idx, split_idx)\n",
    "                if location_key not in user_mapping[user_id]['locations']:\n",
    "                    user_mapping[user_id]['locations'][location_key] = {\n",
    "                        'shard': shard_idx,\n",
    "                        'split': split_idx,\n",
    "                        'rows': []\n",
    "                    }\n",
    "\n",
    "                user_mapping[user_id]['locations'][location_key]['rows'].append(original_index)\n",
    "\n",
    "    #locations dict to the required list format\n",
    "    for user_id in user_mapping:\n",
    "        user_mapping[user_id]['locations'] = list(user_mapping[user_id]['locations'].values())\n",
    "        # Also sort the original_rows list for consistency\n",
    "        user_mapping[user_id]['original_rows'].sort()\n",
    "\n",
    "    print(\"SISA logic with detailed mapping completed successfully.\")\n",
    "    return all_splits, user_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4586bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 & 4: MAIN EXECUTION AND VALIDATION\n",
    "# =======================================================\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
    "    # --- Configuration\n",
    "    INPUT_DATASET = 'diabetes_with_users_reordered.csv'\n",
    "    OUTPUT_DIR = 'sisa_data/'\n",
    "    NUM_SHARDS = 4\n",
    "    SPLITS_PER_SHARD = 3\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\" Directory '{OUTPUT_DIR}' is ready.\")\n",
    "\n",
    "    prepared_df = prepare_dataset(INPUT_DATASET)\n",
    "    all_splits, user_map = create_splits_and_mapping(prepared_df, NUM_SHARDS, SPLITS_PER_SHARD)\n",
    "\n",
    "    print(\"\\n Saving all output files...\")\n",
    "    split_counter = 0\n",
    "    for shard_idx in range(NUM_SHARDS):\n",
    "        for split_idx in range(SPLITS_PER_SHARD):\n",
    "            if split_counter < len(all_splits):\n",
    "                split_df = all_splits[split_counter]\n",
    "                file_name = f'shard_{shard_idx}_split_{split_idx}.csv'\n",
    "                file_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "                split_df.to_csv(file_path, index=False)\n",
    "                split_counter += 1\n",
    "\n",
    "    mapping_file_path = os.path.join(OUTPUT_DIR, 'user_mapping.json')\n",
    "    with open(mapping_file_path, 'w') as f:\n",
    "        json.dump(user_map, f, indent=4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" TEAM 1 EXECUTION COMPLETE \")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"  {split_counter} split files have been saved in the '{OUTPUT_DIR}' folder.\")\n",
    "    print(f\"  User mapping for {len(user_map)} users saved to '{mapping_file_path}'.\")\n",
    "    print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a906785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Directory 'sisa_data/' is ready.\n",
      " Running data preprocessing...\n",
      " Dataset loaded successfully\n",
      "No missing values found\n",
      " No user ID column found. Creating a new 'user_id' column\n",
      "  - Preprocessing complete\n",
      "\n",
      " Running SISA splitting and mapping\n",
      "SISA logic with detailed mapping completed successfully.\n",
      "\n",
      " Saving all output files...\n",
      "\n",
      "==================================================\n",
      " TEAM 1 EXECUTION COMPLETE \n",
      "==================================================\n",
      "  12 split files have been saved in the 'sisa_data/' folder.\n",
      "  User mapping for 442 users saved to 'sisa_data/user_mapping.json'.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0873f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
